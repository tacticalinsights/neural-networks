{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32c12b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "%run activations.ipynb\n",
    "%run optimizers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e692ac1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variable_initialization(L, num_vars=13):\n",
    "    lst = []\n",
    "    for i in range(num_vars):\n",
    "        lst.append({i: 0 for i in range(L + 1)})\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54e2cef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_initialization(L, W, b):\n",
    "    for i in range(1, L+1):\n",
    "        W[i] = np.random.randn(n[i], n[i - 1]) * np.sqrt(2 / n[i - 1]) # xavier initialization\n",
    "        b[i] = np.zeros((n[i], 1))\n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf85b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(L, W, b, Z, A, D, dropout_prob=1):\n",
    "    for i in range(1, L+1):\n",
    "        Z[i] = np.dot(W[i], A[i - 1]) + b[i]\n",
    "        if i != L:\n",
    "            A[i] = relu(Z[i])\n",
    "        else:\n",
    "            A[i] = softmax(Z[i])\n",
    "        # dropout regularization\n",
    "        D[i] = np.random.rand(A[i].shape[0], A[i].shape[1]) < dropout_prob\n",
    "        A[i] = A[i] * D[i]\n",
    "        if dropout_prob != 0:\n",
    "            A[i] /= dropout_prob\n",
    "    return Z, A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d68d9792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(Y, L, m, W, Z, A, dW, db, dZ, dA, D, batch, batch_size, dropout_prob=1):\n",
    "    for i in range(L, 0, -1):\n",
    "        if i == L:\n",
    "            dZ[i] = A[i] - Y[:, batch:batch + batch_size]\n",
    "        else:\n",
    "            dA[i] = np.dot(W[i+1].T, dZ[i+1])\n",
    "            dA[i] *= D[i] # apply dropout mask\n",
    "            if dropout_prob != 0:\n",
    "                dA[i] /= dropout_prob # rescale for dropout\n",
    "            dZ[i] = dA[i] * (Z[i] > 0)\n",
    "        dW[i] = np.dot(dZ[i], A[i-1].T)/m\n",
    "        db[i] = np.sum(dZ[i])/m \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44fe86f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(L, m, W, dW, lambd):\n",
    "    for i in range(1, L + 1):\n",
    "        dW[i] += (lambd/m) * W[i]\n",
    "    return dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a81020a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(X, Y, L, n, epochs, batch_size, optimizer=\"adam\", alpha=0.1, beta1=0.9, beta2=0.999, lambd=0, dropout_prob=1):\n",
    "    m = X.shape[1] # num training examples\n",
    "    W, dW, b, db, Z, dZ, A, dA, D, vdW, vdb, sdW, sdb = variable_initialization(L) # init variables\n",
    "    W, b = weight_initialization(L, W, b) # init weights, biases\n",
    "    losses = [] # store loss values\n",
    "    for epoch in range(epochs): # iterate through epochs\n",
    "        A[0] = X_train # train set\n",
    "        Z, A = forward_prop(L, W, b, Z, A, D) # forward prop\n",
    "        loss = -np.sum(np.log(A[L] + 1e-8) * Y) / m # compute loss\n",
    "        losses.append(loss) # cache loss value\n",
    "        if epoch % (epochs // 10) == 0: # after 10% training progress\n",
    "            print(f\"Epoch {epoch}: {loss}\") # display train loss\n",
    "        for batch in range(0, m, batch_size): # iterate through batches\n",
    "            A[0] = X_train[:, batch:batch + batch_size] # first batch\n",
    "            Z, A = forward_prop(L, W, b, Z, A, D, dropout_prob) # forward prop\n",
    "            dW, db = back_prop(Y, L, m, W, Z, A, dW, db, dZ, dA, D, batch, batch_size, dropout_prob) # back prop\n",
    "            dW = l2_regularization(L, m, W, dW, lambd) # regularization\n",
    "            if optimizer == \"gradient_descent\":\n",
    "                W, b = gradient_descent(W, b, dW, db, alpha) # gradient descent\n",
    "            else:\n",
    "                W, b = adam(W, b, dW, db, vdW, vdb, sdW, sdb, alpha, beta1, beta2) # adam\n",
    "    return W, b, losses"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
